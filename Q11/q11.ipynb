{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question: Are there any specific services or products that are more commonly associated with each reason category for churn? then, How well can we predict the churn reason category based on the services and products that a customer has?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Expectations:\n",
    "analyzing the specific services or products that are commonly associated with each reason category for churn can provide valuable insights for the business. For example, if customers are churning due to issues with internet speed, the company may need to invest in improving their network infrastructure. If customers are churning due to high prices, the company may need to consider adjusting their pricing strategy or offering more affordable packages. By understanding the specific services or products that are driving customer churn, the company can make targeted improvements to reduce churn rates and improve customer satisfaction. Additionally, this information can inform the development of new products or services that better meet the needs and preferences of customers."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### EDA:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the libraries\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_selection import SequentialFeatureSelector\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from imblearn.over_sampling import ADASYN, BorderlineSMOTE\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data from /Dataset/Telco_customer_churn_services.xlsx\n",
    "dataset1 = pd.read_excel('../Dataset/Telco_customer_churn_services.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset1.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_columns = ['Customer ID', 'Phone Service', 'Internet Service', 'Multiple Lines',\n",
    "              'Online Security', 'Online Backup', 'Device Protection Plan', 'Premium Tech Support', 'Unlimited Data']\n",
    "\n",
    "dataset1 = dataset1[my_columns]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need the churn category from a different file\n",
    "dataset2 = pd.read_excel('../Dataset/Telco_customer_churn_status.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset2.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_columns = ['Customer ID', 'Churn Category','Churn Label']\n",
    "\n",
    "dataset2 = dataset2[my_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge the two datasets\n",
    "dataset = pd.merge(dataset1, dataset2, on='Customer ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for messing values\n",
    "dataset.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Customer_ID = dataset['Customer ID']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that churn category is missing some values but we are not sure yet if that is a problem or it is just because the customer did not churn. We will check that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure that when churn category is missing the churn label is false\n",
    "dataset[dataset['Churn Category'].isnull()]['Churn Label'].value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "it is indeed the case that the missing values in the churn category are due to the fact that the customer did not churn. We will drop all customers who have not churned because we are only interested in the customers who have churned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop customers who have not churned\n",
    "dataset = dataset[dataset['Churn Category'].notnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop the customer ID column\n",
    "if 'Customer ID' in dataset.columns:\n",
    "    dataset.drop({'Customer ID','Churn Label'}, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clone the dataset\n",
    "datasetDummies = dataset.copy()\n",
    "# turn the categorical variables into dummy variables except for the churn category \n",
    "dataset = pd.get_dummies(dataset.drop(\n",
    "    'Churn Category', axis=1), drop_first=True)\n",
    "\n",
    "# add the churn category to the dataset\n",
    "dataset['Churn Category'] = datasetDummies['Churn Category']\n",
    "\n",
    "# check the data types of the columns\n",
    "dataset.dtypes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename the columns with _Yes to remove the _Yes\n",
    "if 'Phone Service_Yes' in dataset.columns:\n",
    "    dataset.rename(columns={'Phone Service_Yes': 'Phone Service',\n",
    "                            'Internet Service_Yes': 'Internet Service',\n",
    "                            'Multiple Lines_Yes': 'Multiple Lines',\n",
    "                            'Online Security_Yes': 'Online Security',\n",
    "                            'Online Backup_Yes': 'Online Backup',\n",
    "                            'Device Protection Plan_Yes': 'Device Protection Plan',\n",
    "                            'Premium Tech Support_Yes': 'Premium Tech Support',\n",
    "                            'Unlimited Data_Yes': 'Unlimited Data'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get unique values for churn category\n",
    "dataset['Churn Category'].unique()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### We have four categories of churn reasons:\n",
    "1. Competitor offers\n",
    "2. Price\n",
    "3. Dissatisfaction with service\n",
    "4. Attitude of support person\n",
    "5. Other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize distribution of churn category\n",
    "counts = dataset['Churn Category'].value_counts(\n",
    "    normalize=True).rename('Percentage').mul(100).reset_index()\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.barplot(x='index', y='Percentage', data=counts, width=0.5)\n",
    "plt.title('Distribution of Churn Category')\n",
    "plt.ylabel('Number of Occurrences', fontsize=12)\n",
    "plt.xlabel('Churn Category', fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Insights:\n",
    "1. The most common reason for churn is competitor offers, followed by attitude of support person, dissatisfaction with service, and price."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the distribution of the different services and the churn categories all in one plot\n",
    "# we are only interested in the customers who have the service hence only consider the 1 values\n",
    "\n",
    "# plot the phone service distribution normalized by the total count of customers who have the service\n",
    "plt.figure(figsize=(20, 10))\n",
    "plt.subplot(2, 4, 1)\n",
    "counts = dataset[dataset['Phone Service'] == 1]['Churn Category'].value_counts(\n",
    "    normalize=True).rename('Percentage').mul(100).reset_index()\n",
    "sns.barplot(x='index', y='Percentage', data=counts, width=0.5)\n",
    "plt.title('Phone Service Distribution')\n",
    "plt.ylabel('Percentage')\n",
    "plt.xlabel('Churn Category')\n",
    "\n",
    "# plot the internet service distribution normalized by the total count of customers who have the service\n",
    "plt.subplot(2, 4, 2)\n",
    "counts = dataset[dataset['Internet Service'] == 1]['Churn Category'].value_counts(\n",
    "    normalize=True).rename('Percentage').mul(100).reset_index()\n",
    "sns.barplot(x='index', y='Percentage', data=counts, width=0.5)\n",
    "plt.title('Internet Service Distribution')\n",
    "plt.ylabel('Percentage')\n",
    "plt.xlabel('Churn Category')\n",
    "\n",
    "# plot the multiple lines distribution normalized by the total count of customers who have the service\n",
    "plt.subplot(2, 4, 3)\n",
    "counts = dataset[dataset['Multiple Lines'] == 1]['Churn Category'].value_counts(\n",
    "    normalize=True).rename('Percentage').mul(100).reset_index()\n",
    "sns.barplot(x='index', y='Percentage', data=counts, width=0.5)\n",
    "plt.title('Multiple Lines Distribution')\n",
    "plt.ylabel('Percentage')\n",
    "plt.xlabel('Churn Category')\n",
    "\n",
    "# plot the online security distribution normalized by the total count of customers who have the service\n",
    "plt.subplot(2, 4, 4)\n",
    "counts = dataset[dataset['Online Security'] == 1]['Churn Category'].value_counts(\n",
    "    normalize=True).rename('Percentage').mul(100).reset_index()\n",
    "sns.barplot(x='index', y='Percentage', data=counts, width=0.5)\n",
    "plt.title('Online Security Distribution')\n",
    "plt.ylabel('Percentage')\n",
    "plt.xlabel('Churn Category')\n",
    "\n",
    "# plot the online backup distribution normalized by the total count of customers who have the service\n",
    "plt.subplot(2, 4, 5)\n",
    "counts = dataset[dataset['Online Backup'] == 1]['Churn Category'].value_counts(\n",
    "    normalize=True).rename('Percentage').mul(100).reset_index()\n",
    "sns.barplot(x='index', y='Percentage', data=counts, width=0.5)\n",
    "plt.title('Online Backup Distribution')\n",
    "plt.ylabel('Percentage')\n",
    "plt.xlabel('Churn Category')\n",
    "\n",
    "# plot the device protection plan distribution normalized by the total count of customers who have the service\n",
    "plt.subplot(2, 4, 6)\n",
    "counts = dataset[dataset['Device Protection Plan'] == 1]['Churn Category'].value_counts(\n",
    "    normalize=True).rename('Percentage').mul(100).reset_index()\n",
    "sns.barplot(x='index', y='Percentage', data=counts, width=0.5)\n",
    "plt.title('Device Protection Plan Distribution')\n",
    "plt.ylabel('Percentage')\n",
    "plt.xlabel('Churn Category')\n",
    "\n",
    "\n",
    "# plot the premium tech support distribution normalized by the total count of customers who have the service\n",
    "plt.subplot(2, 4, 7)\n",
    "counts = dataset[dataset['Premium Tech Support'] == 1]['Churn Category'].value_counts(\n",
    "    normalize=True).rename('Percentage').mul(100).reset_index()\n",
    "sns.barplot(x='index', y='Percentage', data=counts, width=0.5)\n",
    "plt.title('Premium Tech Support Distribution')\n",
    "plt.ylabel('Percentage')\n",
    "plt.xlabel('Churn Category')\n",
    "\n",
    "\n",
    "# plot the unlimited data distribution normalized by the total count of customers who have the service\n",
    "plt.subplot(2, 4, 8)\n",
    "counts = dataset[dataset['Unlimited Data'] == 1]['Churn Category'].value_counts(\n",
    "    normalize=True).rename('Percentage').mul(100).reset_index()\n",
    "sns.barplot(x='index', y='Percentage', data=counts, width=0.5)\n",
    "plt.title('Unlimited Data Distribution')\n",
    "plt.ylabel('Percentage')\n",
    "plt.xlabel('Churn Category')\n",
    "\n",
    "plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's reverse it I want to visualize the distribution churn reason across the different services\n",
    "def plot_dist(churn_reason):\n",
    "    # Filter the DataFrame\n",
    "    reason = dataset[dataset['Churn Category']\n",
    "                     == churn_reason].drop(\"Churn Category\", axis=1)\n",
    "    # Reshape the DataFrame to a long format\n",
    "    df_long = reason.melt(var_name='Column', value_name='Value')\n",
    "\n",
    "    # Filter the rows where the value is 1\n",
    "    df_long_ones = df_long[df_long['Value'] == 1]\n",
    "\n",
    "    # Create a box plot\n",
    "    plt.figure(figsize=(20, 10))\n",
    "    sns.countplot(x='Column', data=df_long_ones)\n",
    "    plt.title('Distribution of Churn Reason for ' + churn_reason)\n",
    "    plt.ylabel('Number of Occurrences')\n",
    "    plt.xlabel('Service')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the distribution of churn reason for each churn category \n",
    "plot_dist('Competitor')\n",
    "plot_dist('Attitude')\n",
    "plot_dist('Dissatisfaction')\n",
    "plot_dist('Price')\n",
    "plot_dist('Other')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# turn the churn reason into dummy variables (make a new copy of the dataset)\n",
    "datasetCorr = pd.get_dummies(dataset, columns=['Churn Category'])\n",
    "datasetCorr.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# draw heatmap to visualize the correlation between the different features\n",
    "plt.figure(figsize=(20, 10))\n",
    "sns.heatmap(datasetCorr.corr(method='pearson'), annot=True, fmt='.2f')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# draw heatmap to visualize the correlation between the different features\n",
    "plt.figure(figsize=(20, 10))\n",
    "sns.heatmap(datasetCorr.corr(method='kendall'), annot=True, fmt='.2f')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# draw heatmap to visualize the correlation between the different features\n",
    "plt.figure(figsize=(20, 10))\n",
    "sns.heatmap(datasetCorr.corr(method='spearman'), annot=True, fmt='.2f')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the correlation between the features and the target variable\n",
    "plt.figure(figsize=(20, 20))\n",
    "sns.heatmap(datasetCorr.corr()[['Churn Category_Attitude']].sort_values(\n",
    "    by='Churn Category_Attitude', ascending=False), annot=True, fmt='.2f')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(20, 20))\n",
    "sns.heatmap(datasetCorr.corr()[['Churn Category_Competitor']].sort_values(\n",
    "    by='Churn Category_Competitor', ascending=False), annot=True, fmt='.2f')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(20, 20))\n",
    "sns.heatmap(datasetCorr.corr()[['Churn Category_Dissatisfaction']].sort_values(\n",
    "    by='Churn Category_Dissatisfaction', ascending=False), annot=True, fmt='.2f')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "plt.figure(figsize=(20, 20))\n",
    "sns.heatmap(datasetCorr.corr()[['Churn Category_Other']].sort_values(\n",
    "    by='Churn Category_Other', ascending=False), annot=True, fmt='.2f')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(20, 20))\n",
    "sns.heatmap(datasetCorr.corr()[['Churn Category_Price']].sort_values(\n",
    "    by='Churn Category_Price', ascending=False), annot=True, fmt='.2f')\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to encode the Churn Category column into numerical values so that we can use it in our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['Churn Category'] = dataset['Churn Category'].map(\n",
    "    {'Competitor': 0, 'Attitude': 1, 'Dissatisfaction': 2, 'Price': 3,'Other':4})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the dataset into training and testing sets\n",
    "X = dataset.drop(['Churn Category'], axis=1)\n",
    "y = dataset['Churn Category']\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=7)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Building:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function takes in the model, trains it and evaluates it on the test set\n",
    "# it also uses SequentialFeatureSelector to select the best features for the model\n",
    "def trainModel(model, X_train, y_train, X_test, y_test):\n",
    "\n",
    "    # get features range\n",
    "    feature_range = range(1, len(X_train.columns))\n",
    "\n",
    "    best_model_accuracy = None\n",
    "    best_accuracy = 0.0\n",
    "    best_model_f1 = None\n",
    "    best_f1 = 0.0\n",
    "    best_features = None\n",
    "\n",
    "    # loop through all the features\n",
    "    for i in feature_range:\n",
    "        sfs = SequentialFeatureSelector(\n",
    "            model, n_features_to_select=i, direction='forward')\n",
    "\n",
    "        # train the model using the training sets\n",
    "        sfs.fit(X_train, y_train)\n",
    "\n",
    "        # transform the data sets so that only the selected features are retained\n",
    "        X_train_sfs = sfs.transform(X_train)\n",
    "        X_test_sfs = sfs.transform(X_test)\n",
    "\n",
    "        # Print the selected features\n",
    "        print(\"Selected Features for %d Features: %s\" %\n",
    "            (i, X_train.loc[:, sfs.support_].columns))\n",
    "\n",
    "        # calculate the accuracy of the model using the test sets\n",
    "        model.fit(X_train_sfs, y_train)\n",
    "\n",
    "        # predict the response for the test sets\n",
    "        y_pred = model.predict(X_test_sfs)\n",
    "        #save the best model and best features based on the accuracy\n",
    "        if best_accuracy < accuracy_score(y_test, y_pred):\n",
    "            best_accuracy = accuracy_score(y_test, y_pred)\n",
    "            best_model_accuracy = sfs\n",
    "            best_features = sfs.get_support()\n",
    "\n",
    "        #save the best model and best features based on the f1 score\n",
    "        if best_f1 < f1_score(y_test, y_pred, average='weighted'):\n",
    "            best_f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "            best_model_f1 = sfs\n",
    "            best_features = sfs.get_support()\n",
    "\n",
    "    # Print the selected features based on the accuracy\n",
    "    print(\"Selected Features for Best Accuracy: %s\" %\n",
    "        (X.loc[:, best_model_accuracy.get_support()].columns))\n",
    "\n",
    "    # Print best accuracy\n",
    "    print(\"Best Accuracy: %f\" % (best_accuracy))\n",
    "\n",
    "    # Print the selected features based on the f1 score\n",
    "    print(\"Selected Features for Best F1 Score: %s\" %\n",
    "        (X.loc[:, best_model_f1.get_support()].columns))\n",
    "\n",
    "    # Print best f1 score\n",
    "    print(\"Best F1 Score: %f\" % (best_f1))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the model using Logistic Regression\n",
    "trainModel(LogisticRegression(), X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the model using svm\n",
    "trainModel(SVC(), X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the model using Random Forest\n",
    "trainModel(RandomForestClassifier(), X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Results:\n",
    "1. the model accuracy is very bad which is expected after what we have seen in EDA where all features hd very low correlation with the target variable. So further analysis and features should be added to the model to improve the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the first dataset from ../Dataset/Telco_customer_churn_services.xlsx\n",
    "services = pd.read_excel('../Dataset/Telco_customer_churn_services.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the second dataset from ../Dataset/Telco_customer_churn.xlsx\n",
    "compound = pd.read_excel('../Dataset/Telco_customer_churn.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data from /Dataset/Telco_customer_churn_demographics.xlsx\n",
    "demographics = pd.read_excel(\n",
    "    '../Dataset/Telco_customer_churn_demographics.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename the column to match the column name in the dataset\n",
    "compound.rename(columns={'CustomerID': 'Customer ID'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join the two datasets on the column 'Customer ID'\n",
    "dataset1 = pd.merge(demographics, compound, on='Customer ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the data types of the columns\n",
    "dataset1.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_columns = ['Gender_x', 'Age', 'Married',\n",
    "              'Number of Dependents', 'Churn Value', 'Tenure Months', 'Churn Score', 'Monthly Charges', 'Customer ID']\n",
    "\n",
    "dataset1 = dataset1[my_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.merge(services, dataset1, on='Customer ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_columns += ['Phone Service', 'Internet Service', 'Multiple Lines',\n",
    "               'Online Security', 'Online Backup', 'Device Protection Plan', 'Premium Tech Support', 'Unlimited Data', 'Total Revenue', 'Referred a Friend']\n",
    "\n",
    "dataset = dataset[my_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename Gender_x to gender\n",
    "dataset.rename(columns={'Gender_x' : 'Gender'},inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "status = pd.read_excel('../Dataset/Telco_customer_churn_status.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "status.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['Customer ID', 'Satisfaction Score', 'CLTV']\n",
    "status1 = status[columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.merge(dataset, status1, on='Customer ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Customer_ID = dataset['Customer ID']\n",
    "if 'Customer ID' in dataset.columns:\n",
    "    dataset.drop('Customer ID', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.get_dummies(dataset,drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'Married_Yes' in dataset.columns:\n",
    "    dataset.rename(columns={'Married_Yes':'Married','Gender_Male': 'Gender','Phone Service_Yes':'Phone Service', 'Internet Service_Yes':'Internet Service', 'Multiple Lines_Yes':'Multiple Lines',\n",
    "                        'Online Security_Yes':'Online Security', 'Online Backup_Yes':'Online Backup','Device Protection Plan_Yes':'Device Protection Plan',\n",
    "                        'Premium Tech Support_Yes':'Premium Tech Support','Unlimited Data_Yes':'Unlimited Data','Referred a Friend_Yes':'Referred a Friend'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['Customer ID', 'Churn Category']\n",
    "\n",
    "status2 = status[columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge the two datasets\n",
    "dataset['Customer ID'] = Customer_ID\n",
    "dataset = pd.merge(dataset, status2, on='Customer ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'Customer ID' in dataset.columns:\n",
    "    dataset.drop('Customer ID', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasetCorr = pd.get_dummies(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasetCorr.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#visualize the distribution of the all features in the dataset\n",
    "datasetCorr.hist(figsize=(20, 20))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#visualize the correlation between the features\n",
    "plt.figure(figsize=(20, 20))\n",
    "sns.heatmap(datasetCorr.corr(), annot=True, cmap='coolwarm')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the correlation between the features and the target variable\n",
    "plt.figure(figsize=(20, 20))\n",
    "sns.heatmap(datasetCorr.corr()[['Churn Category_Attitude']].sort_values(\n",
    "    by='Churn Category_Attitude', ascending=False), annot=True, fmt='.2f')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(20, 20))\n",
    "sns.heatmap(datasetCorr.corr()[['Churn Category_Competitor']].sort_values(\n",
    "    by='Churn Category_Competitor', ascending=False), annot=True, fmt='.2f')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(20, 20))\n",
    "sns.heatmap(datasetCorr.corr()[['Churn Category_Dissatisfaction']].sort_values(\n",
    "    by='Churn Category_Dissatisfaction', ascending=False), annot=True, fmt='.2f')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "plt.figure(figsize=(20, 20))\n",
    "sns.heatmap(datasetCorr.corr()[['Churn Category_Other']].sort_values(\n",
    "    by='Churn Category_Other', ascending=False), annot=True, fmt='.2f')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(20, 20))\n",
    "sns.heatmap(datasetCorr.corr()[['Churn Category_Price']].sort_values(\n",
    "    by='Churn Category_Price', ascending=False), annot=True, fmt='.2f')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the dataset into training and testing sets\n",
    "X = dataset.drop(['Churn Category'], axis=1)\n",
    "y = dataset['Churn Category']\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=7)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the model using Logistic Regression\n",
    "trainModel(LogisticRegression(), X_train, y_train, X_test, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
