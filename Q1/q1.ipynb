{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question: How well can we predict customer churn based on demographic features (gender, SeniorCitizen, Partner, and Dependents)?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Expectations\n",
    " This can be a useful question for the telecommunications company, as it can help them to understand which factors are most important in driving customer churn and how they can take targeted actions to reduce churn rates. By analyzing the data and building a predictive model, the company can gain insights into which demographic features are most strongly associated with churn, and use this information to develop retention strategies that are tailored to different customer segments."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Information about the data:\n",
    "The data is stored in an Excel file named `Telco_customer_churn_demographics.xlsx`. The file contains 7043 rows. Each row represents a customer, each column contains customerâ€™s attributes described on the column Metadata. The demographic features that we have are: \n",
    "1. Gender\n",
    "2. Age\n",
    "3. Marriage status\n",
    "4. Dependents"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data from /Dataset/Telco_customer_churn_demographics.xlsx\n",
    "dataset1 = pd.read_excel('../Dataset/Telco_customer_churn_demographics.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need to get a column from another excel file and join it with the dataset\n",
    "\n",
    "# Load the data from /Dataset/Telco_customer_churn.xlsx\n",
    "dataset2 = pd.read_excel('../Dataset/Telco_customer_churn.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename the column to match the column name in the dataset\n",
    "dataset2.rename(columns={'CustomerID':'Customer ID'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join the two datasets on the column 'Customer ID'\n",
    "dataset = pd.merge(dataset1, dataset2, on='Customer ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for any missing values\n",
    "dataset.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure that the merge happened successfully by comparing Gender_x and Gender_y columns to be the same\n",
    "difference = dataset['Gender_x'] != dataset['Gender_y']\n",
    "difference.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the data types of the columns\n",
    "dataset.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop useless columns\n",
    "\n",
    "my_columns = ['Gender_x', 'Age', 'Married',\n",
    "              'Number of Dependents', 'Churn Value']\n",
    "\n",
    "dataset = dataset[my_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# turn the categorical variables into dummy variables\n",
    "dataset = pd.get_dummies(dataset)\n",
    "\n",
    "# check the data types of the columns\n",
    "dataset.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop the newly created dummy variables that are not required\n",
    "if 'Gender_x_Female' in dataset.columns:\n",
    "    dataset = dataset.drop(\n",
    "        ['Gender_x_Female', 'Married_No'], axis=1)\n",
    "\n",
    "\n",
    "# rename the columns to remove the _Yes suffix\n",
    "dataset.rename(columns={'Gender_x_Male': 'Gender',\n",
    "               'Married_Yes': 'Married'}, inplace=True)\n",
    "\n",
    "# check the data types of the columns\n",
    "dataset.dtypes    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Gender = 1 then male if 0 then female "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the head of the dataset\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the dataset into training and test sets\n",
    "train, test = train_test_split(dataset, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the distribution of the demographic features vs churn value\n",
    "fig, ax = plt.subplots(2, 2, figsize=(10, 10))\n",
    "ax[0, 0].hist(train[train['Churn Value'] == 0]['Age'], bins=20, color='blue', alpha=0.5)\n",
    "ax[0, 0].hist(train[train['Churn Value'] == 1]['Age'], bins=20, color='orange', alpha=0.5)\n",
    "ax[0, 0].set_title('Age')\n",
    "\n",
    "ax[0, 1].hist(train[train['Churn Value'] == 0]['Number of Dependents'], bins=10, color='blue', alpha=0.5)\n",
    "ax[0, 1].hist(train[train['Churn Value'] == 1]['Number of Dependents'], bins=10, color='orange', alpha=0.5)\n",
    "ax[0, 1].set_title('Number of Dependents')\n",
    "\n",
    "ax[1, 0].hist(train[train['Churn Value'] == 0]['Gender'],\n",
    "              bins=2, color='blue', alpha=0.5, rwidth=0.8)\n",
    "ax[1, 0].hist(train[train['Churn Value'] == 1]['Gender'],\n",
    "              bins=2, color='orange', alpha=0.5, rwidth=0.8)\n",
    "ax[1, 0].set_title('Gender')\n",
    "\n",
    "ax[1, 1].hist(train[train['Churn Value'] == 0]['Married'],\n",
    "              bins=2, color='blue', alpha=0.5,rwidth=0.8)\n",
    "ax[1, 1].hist(train[train['Churn Value'] == 1]['Married'],\n",
    "              bins=2, color='orange', alpha=0.5, rwidth=0.8)\n",
    "ax[1, 1].set_title('Married')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial insights\n",
    "\n",
    "1. Age seems to be a good predictor of the churn (older people tend to churn more)\n",
    "2. Gender does not seem to have any effect on the churn rate\n",
    "3. Hard to tell wether number of dependents has an effect on churn rate further analysis is needed\n",
    "4. It looks like single people tend to churn more than married people"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform a correlation analysis on the dataset to see which features are highly correlated\n",
    "plt.figure(figsize=(20, 10))\n",
    "sns.heatmap(train.corr(method='pearson'), annot=True, cmap='gist_heat')\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation matrix analysis\n",
    "features seem to be uncorrelated with each other\n",
    "we see positive correlation between churn value and Age\n",
    "we see negative correlation between churn value and Number of dependents, Married\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's check non-linear correlations\n",
    "#sns.pairplot(train, hue='Churn Value')\n",
    "\n",
    "# Correlation with Spearman's Rank Correlation:\n",
    "plt.figure(figsize=(20, 10))\n",
    "sns.heatmap(train.corr(method='spearman'), annot=True, cmap='gist_heat')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation with Kendall's Rank Correlation:\n",
    "plt.figure(figsize=(20, 10))\n",
    "sns.heatmap(train.corr(method='kendall'), annot=True, cmap='gist_heat')\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model building:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's build a logistic regression model\n",
    "\n",
    "# drop the target variable from the training set\n",
    "X_train = train.drop('Churn Value', axis=1)\n",
    "\n",
    "# select the target variable from the training set\n",
    "y_train = train['Churn Value']\n",
    "\n",
    "# drop the target variable from the test set\n",
    "X_test = test.drop('Churn Value', axis=1)\n",
    "\n",
    "# select the target variable from the test set\n",
    "y_test = test['Churn Value']\n",
    "\n",
    "# balance the training set\n",
    "sm = SMOTE(random_state=42)\n",
    "X_train, y_train = sm.fit_resample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define hyperparameter values to be tuned\n",
    "param_grid = {'penalty': ['l1', 'l2'],\n",
    "              'C': [0.01, 0.1, 1],\n",
    "              'solver': ['liblinear', 'saga'],\n",
    "              'class_weight': [None, 'balanced']}\n",
    "\n",
    "# Create a logistic regression model\n",
    "logreg = LogisticRegression()\n",
    "\n",
    "# Create a GridSearchCV object to perform hyperparameter tuning\n",
    "grid_search = GridSearchCV(logreg, param_grid, cv=5, scoring='accuracy',n_jobs=10)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best hyperparameter\n",
    "best_params = grid_search.best_params_\n",
    "print(\"Best Hyperparameters: \", best_params)\n",
    "\n",
    "# Fit the logistic regression model with best hyperparameter to the training data\n",
    "best_logreg = LogisticRegression(**best_params)\n",
    "best_logreg.fit(X_train, y_train)\n",
    "\n",
    "# Predict the churn value for the test set\n",
    "y_pred = best_logreg.predict(X_test)\n",
    "\n",
    "# Evaluate the model performance\n",
    "report = classification_report(y_test, y_pred)\n",
    "print(report)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic regression results analysis\n",
    "\n",
    "very poor accuracy of 56% on the test set\n",
    "we see that the model is not able to predict the churn rate of the customers based on the demographic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's build a random forest model\n",
    "\n",
    "# Define hyperparameter values to be tuned\n",
    "param_grid = {\n",
    "    \"n_estimators\": [100, 500],\n",
    "    \"max_depth\": [5, 10],\n",
    "    \"min_samples_split\": [2, 5],\n",
    "    \"min_samples_leaf\": [1, 2],\n",
    "}\n",
    "\n",
    "# Create a random forest model\n",
    "rf = RandomForestClassifier()\n",
    "\n",
    "# Create a GridSearchCV object to perform hyperparameter tuning\n",
    "grid_search = GridSearchCV(rf, param_grid, cv=5, scoring='accuracy',n_jobs=10)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best hyperparameter\n",
    "best_params = grid_search.best_params_\n",
    "print(\"Best Hyperparameters: \", best_params)\n",
    "\n",
    "# Fit the random forest model with best hyperparameter to the training data\n",
    "best_rf = RandomForestClassifier(**best_params)\n",
    "best_rf.fit(X_train, y_train)\n",
    "\n",
    "# Predict the churn value for the test set\n",
    "y_pred = best_rf.predict(X_test)\n",
    "\n",
    "# Evaluate the model performance\n",
    "report = classification_report(y_test, y_pred)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's build a SVM model\n",
    "\n",
    "# Define hyperparameter values to be tuned\n",
    "param_grid = {\n",
    "    \"C\": [0.1, 1, 10],\n",
    "    \"kernel\": [\"linear\", \"rbf\"],\n",
    "    \"gamma\": [\"scale\", \"auto\"]\n",
    "}\n",
    "\n",
    "# Create a SVM model\n",
    "svm = SVC()\n",
    "\n",
    "# Create a GridSearchCV object to perform hyperparameter tuning\n",
    "grid_search = GridSearchCV(svm, param_grid, cv=5, scoring='accuracy',n_jobs=10)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best hyperparameter\n",
    "best_params = grid_search.best_params_\n",
    "print(\"Best Hyperparameters: \", best_params)\n",
    "\n",
    "# Fit the SVM model with best hyperparameter to the training data\n",
    "best_svm = SVC(**best_params)\n",
    "best_svm.fit(X_train, y_train)\n",
    "\n",
    "# Predict the churn value for the test set\n",
    "y_pred = best_svm.predict(X_test)\n",
    "\n",
    "# Evaluate the model performance\n",
    "report = classification_report(y_test, y_pred)\n",
    "print(report)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results Interpretation:\n",
    "Based on the results of the three models, it appears that predicting customer churn based solely on demographic features (gender, SeniorCitizen, Partner, and Dependents) is challenging. The best model achieved an accuracy of around 57%, which is only slightly better than random guessing.\n",
    "\n",
    "The Logistic Regression model had the highest recall score for predicting customer churn (0.78), which means it correctly identified 78% of customers who were likely to churn. However, its precision score for predicting customer churn was low (0.37), which means it also identified a large number of false positives.\n",
    "\n",
    "The Random Forest and SVM models had similar results with an accuracy of around 56%, and relatively balanced precision and recall scores for predicting customer churn.\n",
    "\n",
    "Based on these results, it seems that demographic features alone may not be sufficient to accurately predict customer churn. To improve the accuracy of the predictive model, it may be necessary to include additional features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the dataset as an Excel file to use for tableau visualization\n",
    "dataset.to_excel('q1.xlsx', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
